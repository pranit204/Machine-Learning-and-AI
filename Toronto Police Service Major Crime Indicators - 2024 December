# -*- coding: utf-8 -*-
"""
Latest Dataset Download Date: ***December 20, 2024***

Dataset Link: https://data.torontopolice.on.ca/datasets/0a239a5563a344a3bbf8452504ed8d68_0/explore?location=16.379014%2C-39.819624%2C2.62

Documentation Link: https://data.torontopolice.on.ca/datasets/TorontoPS::major-crime-indicators-open-data/about
"""

# importing all required machine learning libraries and packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.cluster import KMeans, DBSCAN
from statsmodels.tsa.arima.model import ARIMA
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM

# dataset

df=pd.read_csv('/Users/pranitsanghavi/Downloads/Major_Crime_Indicators_Open_Data_-3805566126367379926.csv')

# latest date in dataset
df['REPORT_DATE'].max()

"""**Data Transformations and Explorations**"""

print(df.head())

# ensuring correct columns are used
print(df.columns.tolist())

# only considering the columns we need
df=df[['OBJECTID','REPORT_YEAR',
       'REPORT_MONTH', 'REPORT_DAY', 'REPORT_DOY', 'REPORT_DOW', 'REPORT_HOUR',
       'OCC_YEAR', 'OCC_MONTH', 'OCC_DAY', 'OCC_DOY', 'OCC_DOW', 'OCC_HOUR','DIVISION', 'LOCATION_TYPE', 'PREMISES_TYPE',
       'OFFENCE', 'MCI_CATEGORY','NEIGHBOURHOOD_158','LONG_WGS84', 'LAT_WGS84' ]]

print(df.shape)

# checking column types before we change the formatting of some of the columns
print(df.dtypes)

# comparing column data type for occ_year vs. report_year
print(df[['OCC_YEAR','REPORT_YEAR']].sort_values(by='OCC_YEAR'))

# grouping total count of instances by year
print(df['OCC_YEAR'].value_counts().sort_index())

# checking for missing values
print(df.isna().sum())

# filtering dataframe for data only after 2014
df=df[df['OCC_YEAR']>=2014]

# Replace missing OCC_YEAR, OCC_MONTH, and OCC_DAY with corresponding REPORT_YEAR, REPORT_MONTH, REPORT_DAY
df['OCC_YEAR'] = df['OCC_YEAR'].fillna(df['REPORT_YEAR'])
df['OCC_MONTH'] = df['OCC_MONTH'].fillna(df['REPORT_MONTH'])
df['OCC_DAY'] = df['OCC_DAY'].fillna(df['REPORT_DAY'])

# Convert month names to numbers
df['REPORT_MONTH'] = pd.to_datetime(df['REPORT_MONTH'], format='%B').dt.month
df['OCC_MONTH'] = pd.to_datetime(df['OCC_MONTH'], format='%B').dt.month

# ensuring date column types that we need are formatted correctly
df['REPORT_YEAR'] = df['REPORT_YEAR'].astype(int)
df['REPORT_MONTH'] = df['REPORT_MONTH'].astype(int)
df['REPORT_DAY'] = df['REPORT_DAY'].astype(int)
df['REPORT_HOUR'] = df['REPORT_HOUR'].astype(int)
df['OCC_YEAR'] = df['OCC_YEAR'].astype(int)
df['OCC_MONTH'] = df['OCC_MONTH'].astype(int)
df['OCC_DAY'] = df['OCC_DAY'].astype(int)
df['OCC_HOUR'] = df['OCC_HOUR'].astype(int)

# Combine into a datetime column
df['REPORT_DATETIME'] = df.apply(
    lambda row: pd.Timestamp(
        year=row['REPORT_YEAR'],
        month=row['REPORT_MONTH'],
        day=row['REPORT_DAY'],
        hour=row['REPORT_HOUR']
    ),
    axis=1
)

df['OCCURED_DATETIME'] = df.apply(
    lambda row: pd.Timestamp(
        year=row['OCC_YEAR'],
        month=row['OCC_MONTH'],
        day=row['OCC_DAY'],
        hour=row['OCC_HOUR']
    ),
    axis=1
)

# Top 20 neighborhoods with the most number of instances reported (these are the most notorious)
print(df['NEIGHBOURHOOD_158'].value_counts().sort_values(ascending=False).head(20))

# counting number of unique neighborhoods
print(df['NEIGHBOURHOOD_158'].nunique())

# number of offence types
print('Number of offence types:',df['OFFENCE'].nunique())

# count of all offences
print(df['OFFENCE'].value_counts().sort_values(ascending=False))

# checking the average of the difference in days between occ_date and report_date
df['Days_Difference'] = (df['REPORT_DATETIME'] - df['OCCURED_DATETIME']).dt.days
print('Mean Days Difference:',df['Days_Difference'].mean())
print('Median Days Difference:',df['Days_Difference'].median())
print('Max Days Difference:',df['Days_Difference'].max())
print('Min Days Difference:',df['Days_Difference'].min())

#pivot table for offences by year and sorting by total count
offence_pivot = df.pivot_table(index='OFFENCE', columns='OCC_YEAR', values='OBJECTID', aggfunc='count')
offence_pivot['Total'] = offence_pivot.sum(axis=1)
offence_pivot.sort_values(by='Total', ascending=False, inplace=True)
print(offence_pivot)

# same table as above but numbers represent % of total now
offence_pivot_percent = offence_pivot.div(offence_pivot['Total'], axis=0) * 100
print(round(offence_pivot_percent,2))

# same table as above but now with percent growth
# Sort by Total column in descending order
offence_pivot.sort_values(by='Total', ascending=False, inplace=True)
percent_growth = offence_pivot.drop(columns=['Total']).pct_change(axis=1) * 100
percent_growth = percent_growth.fillna(0)
percent_growth = round(percent_growth, 2)
print(percent_growth)

# checking if a particular month stands out for the offence types, numbers should be % of total for the offence type

offence_pivot_month = df.pivot_table(index='OFFENCE', columns='OCC_MONTH', values='OBJECTID', aggfunc='count')
offence_pivot_month['Total'] = offence_pivot_month.sum(axis=1)
offence_pivot_month = offence_pivot_month.div(offence_pivot_month['Total'], axis=0) * 100
offence_pivot_month.sort_values(by='Total', ascending=False, inplace=True)
offence_pivot_month.drop(columns=['Total'], inplace=True)
print(round(offence_pivot_month,2))

# Top 5 neighborhoods with assault
assault_pivot_top5=df[df['OFFENCE']=='Assault'].pivot_table(index='NEIGHBOURHOOD_158', values='OBJECTID', aggfunc='count')
assault_pivot_top5.sort_values(by='OBJECTID', ascending=False, inplace=True)
assault_pivot_top5=assault_pivot_top5.head()
print(assault_pivot_top5)

# Top 5 neighborhoods with assault with weapon
assault_weapon_pivot_top5=df[df['OFFENCE']=='Assault With Weapon'].pivot_table(index='NEIGHBOURHOOD_158', values='OBJECTID', aggfunc='count')
assault_weapon_pivot_top5.sort_values(by='OBJECTID', ascending=False, inplace=True)
assault_weapon_pivot_top5=assault_weapon_pivot_top5.head()
print(assault_weapon_pivot_top5)

# top 5 neighborhoods for theft of motor vehicle
theft_motor_pivot_top5=df[df['OFFENCE']=='Theft Of Motor Vehicle'].pivot_table(index='NEIGHBOURHOOD_158', values='OBJECTID', aggfunc='count')
theft_motor_pivot_top5.sort_values(by='OBJECTID', ascending=False, inplace=True)
theft_motor_pivot_top5=theft_motor_pivot_top5.head()
print(theft_motor_pivot_top5)

# top 5 neighborhoods for break and enter
break_enter_pivot_top5=df[df['OFFENCE']=='B&E'].pivot_table(index='NEIGHBOURHOOD_158', values='OBJECTID', aggfunc='count')
break_enter_pivot_top5.sort_values(by='OBJECTID', ascending=False, inplace=True)
break_enter_pivot_top5=break_enter_pivot_top5.head()
print(break_enter_pivot_top5)

# Correlation between time of day and the number of incidences

# Grouping by REPORT_HOUR and count the number of instances
hourly_counts = df.groupby('OCC_HOUR')['OBJECTID'].count().reset_index()

# Renaming columns for clarity
hourly_counts.columns = ['Hour', 'Instance_Count']

# Calculate correlation between time of day and number of instances
correlation = hourly_counts['Hour'].corr(hourly_counts['Instance_Count'])
print(f"Correlation between time of day and number of instances: {correlation:.2f}")

plt.figure(figsize=(10, 6))
plt.scatter(hourly_counts['Hour'], hourly_counts['Instance_Count'], alpha=0.7)
plt.title("Number of Instances vs. Time of Day")
plt.xlabel("Hour of Day")
plt.ylabel("Number of Instances")
plt.grid()
plt.show()

# checking if b&e and theft of motor vehicle occur more at specific times of day
# Grouping by report_hour and the number of instances of b&e
b_e_counts = df[df['OFFENCE'] == 'B&E'].groupby('OCC_HOUR')['OBJECTID'].count().reset_index()
b_e_counts.columns = ['Hour', 'B&E_Count']
b_e_counts.sort_values(by='B&E_Count', ascending=False, inplace=True)
print(b_e_counts)

# checking if theft of motor vehicle occur more at specific times of day
# Grouping by report_hour and the number of instances
theft_motor_counts = df[df['OFFENCE'] == 'Theft Of Motor Vehicle'].groupby('OCC_HOUR')['OBJECTID'].count().reset_index()
theft_motor_counts.columns = ['Hour', 'Theft_Count']
theft_motor_counts.sort_values(by='Theft_Count', ascending=False, inplace=True)
print(theft_motor_counts)

"""**Machine Learning**

Task 1 - Crime Hotspot Detection (Clustering)
"""

# Filter relevant columns
geo_features = ['LAT_WGS84', 'LONG_WGS84']
df_geo = df.dropna(subset=geo_features)

# Identify rows with non-usable latitude and longitude
non_usable_lats_longs = df_geo[
    (df_geo['LAT_WGS84'] == 0) | (df_geo['LONG_WGS84'] == 0) |  # Zero values
    (df_geo['LAT_WGS84'] < -90) | (df_geo['LAT_WGS84'] > 90) |  # Out-of-range latitude
    (df_geo['LONG_WGS84'] < -180) | (df_geo['LONG_WGS84'] > 180)  # Out-of-range longitude
]

# Count and display the number of non-usable rows
non_usable_count = non_usable_lats_longs.shape[0]
print(f"Number of non-usable rows: {non_usable_count}")

# Remove non-usable rows
geo_data_cleaned = df_geo.drop(non_usable_lats_longs.index)

# Extract cleaned latitude and longitude for clustering
geo_data = geo_data_cleaned[['LAT_WGS84', 'LONG_WGS84']]

# Apply DBSCAN clustering
dbscan = DBSCAN(eps=0.01, min_samples=50, metric='euclidean')
dbscan.fit(geo_data)

# Add cluster labels to the dataset
geo_data_cleaned['Cluster'] = dbscan.labels_

# Visualize the clusters
plt.figure(figsize=(12, 8))
plt.scatter(
    geo_data_cleaned['LONG_WGS84'],
    geo_data_cleaned['LAT_WGS84'],
    c=geo_data_cleaned['Cluster'],
    cmap='tab10',
    s=1,
    alpha=0.6
)
plt.title("Crime Hotspots Identified by DBSCAN", fontsize=16)
plt.xlabel("Longitude", fontsize=12)
plt.ylabel("Latitude", fontsize=12)
plt.colorbar(label="Cluster ID")
plt.grid(True)
plt.show()

# Save the dataset with cluster labels for later use
geo_data_cleaned.to_csv('crime_clusters.csv', index=False)

"""Task 2 - Predicting the type of Offence given other dimensions such as neighborhood, date and time, location and premises type"""

# Define features and target
features = [
    'OCC_YEAR', 'OCC_HOUR', 'OCC_DAY', 'OCC_MONTH', 'OCC_DOW',
    'NEIGHBOURHOOD_158', 'DIVISION', 'LOCATION_TYPE', 'PREMISES_TYPE', 'MCI_CATEGORY'
]
target = 'OFFENCE'

# Prepare the dataset
df_ml = df[features + [target]]

# Drop rows with missing OCC fields
df_ml = df_ml.dropna(subset=['OCC_YEAR', 'OCC_HOUR', 'OCC_DAY', 'OCC_MONTH', 'OCC_DOW', 'NEIGHBOURHOOD_158'])

# Remove rare classes with fewer than 2 occurrences
class_counts = df_ml[target].value_counts()
valid_classes = class_counts[class_counts >= 2].index
df_ml = df_ml[df_ml[target].isin(valid_classes)]

# Split data into features and target
X = df_ml.drop(columns=[target])
y = df_ml[target]

# Encode target variable
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Train-test split with stratification
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# Preprocessing: Encode categorical features and scale numerical features
categorical_features = ['OCC_DOW', 'NEIGHBOURHOOD_158', 'DIVISION', 'LOCATION_TYPE', 'PREMISES_TYPE', 'MCI_CATEGORY']
numerical_features = ['OCC_YEAR', 'OCC_HOUR', 'OCC_DAY']

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),
        ('num', StandardScaler(), numerical_features)
    ]
)
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Train a Logistic Regression model
clf = LogisticRegression(multi_class='multinomial', max_iter=1000, random_state=42)
clf.fit(X_train_processed, y_train)

# Predict on test data
y_pred_encoded = clf.predict(X_test_processed)
y_pred = label_encoder.inverse_transform(y_pred_encoded)

# Decode test labels for evaluation
y_test_decoded = label_encoder.inverse_transform(y_test)

# Evaluate the model
accuracy = accuracy_score(y_test_decoded, y_pred)
classification_rep = classification_report(y_test_decoded, y_pred, zero_division=0)
conf_matrix = confusion_matrix(y_test_decoded, y_pred)

# Display evaluation metrics
print("Accuracy:", accuracy)
print("\nClassification Report:\n", classification_rep)
print("\nConfusion Matrix:\n", conf_matrix)





"""Task 3 - Temporal Crime Analysis"""

# Aggregate crime counts by year and month
crime_trends_year = df.groupby('OCC_YEAR').size().reset_index(name='Crime_Count')
crime_trends_month = df.groupby(['OCC_YEAR', 'OCC_MONTH']).size().reset_index(name='Crime_Count')

# Plot yearly crime trends
plt.figure(figsize=(10, 6))
plt.plot(crime_trends_year['OCC_YEAR'], crime_trends_year['Crime_Count'], marker='o')
plt.title('Crime Trends by Year', fontsize=16)
plt.xlabel('Year', fontsize=12)
plt.ylabel('Number of Crimes', fontsize=12)
plt.grid(True)
plt.show()

# Plot monthly crime trends (grouped by year)
plt.figure(figsize=(12, 8))
for year, group in crime_trends_month.groupby('OCC_YEAR'):
    plt.plot(group['OCC_MONTH'], group['Crime_Count'], marker='o', label=f'Year {int(year)}')

plt.title('Crime Trends by Month (Grouped by Year)', fontsize=16)
plt.xlabel('Month', fontsize=12)
plt.ylabel('Number of Crimes', fontsize=12)
plt.legend(title="Year", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.show()
